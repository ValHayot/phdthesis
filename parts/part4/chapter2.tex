\chapter{Automated neuroimaging data management with the Sea filesystem}

Val\'erie Hayot-Sasson and Tristan Glatard \\
\begingroup \footnotesize
Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada \\
\endgroup 
\vspace{5pt} \\
In preparation: \\

\section{Abstract}
	Neuroimaging open-data initiatives have led to the availability of large scientific datasets. These voluminous
	datasets provide researchers with new insights into the human brain across diverse populations. Difficulties with
	managing such large data has partially hindered the advancement of scientific studies analyzing these datasets. Many
	software tools and strategies have emerged to facilitate the processing. Many open datasets have been stored on cloud storage
	services, such as AWS, enabling rapid transfer of the data at a cost. Software tools such as Datalad has facilitated both the
	sharing and versioning of data. Despite these advancements in-analysis data-management remains limited in neuroimaging workflows.
    
	Neuroimaging workflows, particularly preprocessing ones, may lead to a magnification in output sizes due to <add reason>. They also produce
	significant amounts of intermediary data due to being composed of a multitude of steps. 
    
    \section{Introduction}\label{sec:sea_neuro:introduction}
    
    Neuroimaging datasets continue to grow in size resulting in new challenges related
    to data management. The current largest neuroimaging datasets, such as the Human Connectome Project (HCP)~\cite{HCP}
    and the UK Biobank~\cite{ukbiobank} reach up to Petabytes of data. Big Data in neuroimaging can be present in two formats: 1)
    very large files, such as those found in the BigBrain~\cite{bigbrain}, and 2) large datasets made up of very small files,
    such as those typically found in fMRI datasets. 
    
    When processing, large datasets can result in longer processing times even when compute resources are ample.
    This is a direct result of the underlying storage used by the applications to read and write data. Since large
    datasets require a significant amount of available storage space, more often than not, slower but larger storage is
    selected, resulting in longer data transfer times during processing. With the Sea filesystem, we aim to
    leverage all available storage in order to reduce overall data transfer time during pipeline execution.
    
    Many researchers rely on one of two systems to meet their storage and computing needs: 1) High Performance 
    Computing (HPC) clusters and 2) the cloud. Whereas the cloud simplifies data sharing and gives researchers
    access to a wide variety of infrastructures, HPC clusters are a cost-effective solution to accessing a wide
    array of resources for researchers. In this paper, we will focus on HPC processing.
    
    HPC systems rely on scalable network-based parallel filesystems (e.g. Lustre) for storage. While such file
    systems offer excellent performance, they are shared between all the users on the cluster. Meaning that users
    with data-intensive can effectively deteriorate the performance of the shared file system for all users on
    the cluster. Solutions for improving shared file system performance include throttling the data intensive workloads
    or recommending the use of Burst Buffers~\cite{bb} (e.g. reserving a compute node for storage or leveraging
    local compute storage during processing). The latter, however, requiring that the user manages their data to
    and from the Burst Buffer.
    
    Leveraging local storage to improve data intensive workload performance has long since existed in Big Data
    frameworks such as Apache Spark~\cite{spark} and Dask~\cite{dask}. While these frameworks have been used to
    process neuroimaging data~\cite{manypapers}, they remain seldom used as their require rewriting existing
    standard neuroimaging tools (e.g FSL, AFNI, SPM) for the framework. Although it is possible to use the
    tools within the Big Data frameworks, optimizations like in-memory computing would not be leveraged due to 
    the fact that neuroimaging tools are meant to be used as command-line tools and do not provide interfaces that 
    enable the data to be transferred in-memory.
    
    Frameworks used in neuroimaging, such as NiPype~\cite{nipype} and Joblib~\cite{joblib} instead focus on reducing compute times of workloads.
    This is because even with large datasets, neuroimaging data processing remains split between compute and data
    intensive components. Although these frameworks do not prohibit the use of intelligent data management, it is not directly integrated
    into the workflow. In order to give neuroimaging applications data management capabilities, the applications must interact with a
    file system that can do so.
    
    In order for a file system to be usable by the average researcher on an HPC system, they must be loadable without administrative
    privileges. Furthermore, as the applications are typically made to interact with POSIX-based file systems, the new file system must
    also be compliant to the format. One method to ensure that these conditions are met is by using the \texttt{LD\_PRELOAD} trick. This trick is
    used to intercept select library calls and redefine their behaviour. It has been used in many projects and to create lightweight versions
    of filesystems~\cite{xtreemfs}. 
    
    In this paper, we present Sea, a file system designed to automate data management of neuroimaging pipelines running on HPC systems.
    The goal of Sea is to complement existing neuroimaging applications such that both efficient compute and data management can be achieved
    executed on HPC without significant user intervention. As a secondary goal, Sea aims to ensure the fair sharing of cluster usage
    by alleviating the impacts of heavy writers on the shared file systems as a whole. 
    
    
    
    % \section{Related Work}
    
    % Put in intro.
    % summary of computing landscape for neuroimaging.
    % HCP, CBRAIN, nipype, joblib. sea can be used in all these contexts.
    
    
    \section{Materials and Methods}
    
    \subsection{The Sea filesystem}
    
    The Sea file system is an on-demand file system that leverages the \texttt{LD\_PRELOAD} trick to intercept POSIX file system (more specifically, glibc)
    calls on Linux systems. This enables Sea to redirect write calls aimed for a slower storage device to a faster device whenever
    possible. Similarly, when intercepting read calls, Sea can choose to read from a faster device if a copy is available on that device.
    Sea decides which storage location it can write to based on the detailed provided in an initialization file called \texttt{sea.ini}. 
    This file informs Sea of which locations it can use to read and write to, and well as their order of priority. An example of the initialization
    file can be seen in ~\ref{seaini}.
    
    As neuroimaging pipeline results are typically required post-processing and HPC compute-local resources are only accessible during the
    reserved duration, Sea provides functionality to flush and evict data to persistent shared storage. This is accomplished via a separate
    thread (known as the ``flusher'') that moves data from the caches to long-term storage. A separate thread is used, in this case, to avoid
    interrupting on going processing with data management operations.
    Users must inform Sea of files that need to be
    persisted to storage within a file called \texttt{.sea\_flushlist}, and temporary files which can be removed from cache within a file
    called the \texttt{.sea\_evictlist}. Both these files can be populated using regular expressions to denote the paths to flush and evict.
    If a file occurs in both the \texttt{.sea\_flushlist} and \texttt{.sea\_evictlist}, Sea will interpret this as a \texttt{move} operation
    and simply move the file from local to persistent storage rather than copying it. Files that will be reused by the application should only
    be flushed rather than flushed and evicted, as files can benefit from speedups related to reading from fast rather than slow storage.
    Sea currently also provides a rudimentary prefetch thread that can move files located within the Sea filesystem to the fastest available cache.
    To use Sea's prefetch capabilities, a file called \texttt{.sea\_prefetchlist} needs to be populated using regular expressions like the
    flushing and eviction files.
    
    To interact with the Sea file system, a mountpoint is used. The mountpoint is an empty directory that behaves as a view to
    all the files and directories stored within Sea. In order to keep track of the locations of the files within the mountpoint, Sea
    mirrors the file structure of each storage location across all storage locations. That means, it is generally advisable to provide
    empty storage locations for Sea to write to as the mirroring of large directories can take some time. When prefetching, it is
    best to provide Sea with a path that only points to that data the requires prefetching as mirroring the paths of full datasets may 
    extend processing time.
    
    Sea can easily be launched directly using the available containers on the GitHub registry, or can be compiled via source using Make.
    It requires a version of glibc greater than X. Once compiled, Sea can be executed using the `sea-app.sh' binary.
    
    \subsection{Performance analysis of Sea for neuroimaging}
    
    To determine the performance gain Sea brings to neuroimaging analyses, we must evaluate the value of Sea on a variety of neuroimaging
    applications. For our analysis, we selected different fMRI preprocessing applications as fMRI processing has some of the most
    well-established tools for neuroimaging and some of the largest datasets. Of course, different modalities an tools may result in
    vastly different data access patterns and compute times, we use fMRI prepreprocessing as our baseline for the benefits that can be
    obtained with Sea. 
    
    
    
    \subsubsection{Datasets}
    fMRI datasets may vary greatly in total number
    of images and number of volumes within each image. To adequately capture the 
    diversity of datasets and the applicability of Sea, we have selected three datasets of varying
    time and space resolutions (Table~\ref{table:data}): 1) OpenNeuro's ds001545 dataset~\cite{ds001545},
    2) the PREVENT-AD dataset~\cite{preventad}, and 3) the HCP dataset~\cite{HCP}.
    The ds001545 dataset is a total of 45.94~$GB$ (1778 files) and consists of data collected
    from 30 participants in a single session watching 3 different clips (i.e. three runs/participants) of the The Grand Budapest Hotel.
    
    
    The PREVENT-AD dataset is publicly available dataset comprising of data from 330 participants with a familial history
    of Alzheimer's Disease.
    At the time of the experiments, the dataset contained 255.0~$GB$ (53061 files) originating from the 308 subjects available on DataLad~\cite{}.
    
    The HCP project, whose aim is to characterize brain connectivity using data collected from 1200 subjects, is the largest of the three datasets.
    At the time of our experimments, the dataset obtained consisted 85.4~$TB$ (x files) of data collected from 1113 subjects.
    
    By experimenting with datasets of different spatial and temporal resolutions, the amount of compute may be affected.
    Naturally, with more data there may inevitably be more computations performed, but also, the code itself may
    chose to perform entirely different computations as a result of differences in resolution. Moreover, parallelism and program I/O behaviour
    may be impacted depending on the particular characteristics of the data. Capturing this potential
    variety in processing will provide us with a better understanding of when and where Sea can be useful.
    
    \begin{table*}[t]
      \small\centering
    \begin{tabular}{|c c c c c|}
      \hline
      Dataset & Total Size (MB) & Total Number of images & Number of images processed & Total size of processed (MB) \\
      \hline
      \multirow{3}{*}{PREVENT-AD} & \multirow{3}{*}{289532} & \multirow{3}{*}{53061} & 1 & 51.26\\
      & & & 8 & \\
      & & & 16 & \\
      \hline
      \multirow{3}{*}{ds001545} & \multirow{3}{*}{27377} & \multirow{3}{*}{1778} & 1 & 280.27\\
      & & & 8 & \\
      & & & 16 & \\
      \hline
      \multirow{3}{*}{HCP} & \multirow{3}{*}{83140079} & \multirow{3}{*}{X} & 1 &  1299.93\\
      & & & 8 & \\
      & & & 16 & \\
    
    
      \hline
    
      \hline
    \end{tabular}\caption{Dataset characteristics}\label{table:data}
    \end{table*}
    
    %just for example run
    %dataset size, resolution, resolution of voxels
    %matrix size pixdim1X2
    %number of slices pixdim
    %number of volumes pixdim4
    %inplane resolution pixsize1x2
    %slice thickness pizsize3 in mm
    %repetition time pixsize4 in s
    
    %HCP resting state + x stats
    
    %# files written and output size, duration and memory consumption.
    \subsubsection{fMRI Preprocessing Pipelines}
    
    Similarly to datasets, preprocessing pipelines can also vary in duration as a result of methodological differences.
    Thus, we preprocessed each dataset using four standard preprocessing pipelines: FSL~\cite{fsl}, AFNI~\cite{AFNI},
    SPM~\cite{spm}, fMRIPrep~\cite{fmriprep}. Table~\ref{table:pipelines} provides an reference of the differences in computation and data-intensivity
    of the different pipelines on a single subject of all three of the datasets.
    
    Each tool was set to only run the preprocessing pipeline. Scripts used to execute each pipeline can be found at~\ref{github.com/valhayot/sea-slurm-scripts}.
    
    For the FSL preprocessing pipeline, the default settings were maintained with the exception of slice-timing correction (interleaved),
    intensity normalization and non-linear registration to standard space.
    
    %% Add SPM here
    To preprocess the datasets with SPM12, we reused the template described in \cite{haitas2021}, with fieldmap correction removed.
    As the template was designed for a specific dataset, we updated the paths to the fMRI and anatomical images,
    as well as the dimensions, such as the number of volumes and slices and the TR and TA, for each dataset. Furthermore,
    we set the slice order to interleaved in all cases. 
    
    For AFNI preprocessing, the pipeline was configured to perform slice timing, alignment to Talaraich space, registration, smoothing
    and brain masking. While AFNI does have some parallelized components that can be controlled via environment variables, we chose to let 
    the pipeline use as much parallelism as required for its execution.
    
    We set fMRIPrep to run with \texttt{--fs-no-reconall} flag set and set the \texttt{--bids-database-dir} to a tmpfs location. 
    Furthermore, the seeds were fixed to ensure reproducible results. Unlike the other tools where only a single fMRI image was preprocessed
    per process, it was not possible to do so with fMRIPrep. Thus, fMRIPrep preprocessed entire subject directories at a time
    
    %%{provide subject #/session#} of the ds001545 dataset.
    
    \begin{table*}[t]
      \small\centering
    \begin{tabular}{|c c c c c c|}
      \hline
      Tool & Dataset &  Output Size (MB) & Total libc calls & Libc Lustre calls & Compute time (s) \\
      \hline
      \multirow{3}{1em}{AFNI} & PREVENT-AD & 539.6 & 272390 & 4348 & 103.25 \\
      & ds001545 & 3063.4 & 281707 & 4570  & 280.30 \\
      & HCP & 18720.4 & 305310 & 5367 & 816.16 \\
      \hline
      \multirow{3}{1em}{FSL Feat} & PREVENT-AD & 253.7 & &  & 1338.29 \\
      & ds001545 & 550.6 & &  & 2145.96 \\
      & HCP & 1607.9 & & & 6596.46 \\
      \hline
      \multirow{3}{1em}{SPM} & PREVENT-AD & 331.0 & 34868 & 10048 &  483.67 \\
      & ds001545 & 744.4 & 47251 & 19568 & 446.53 \\
      & HCP & 2082.5 & 54753 & 25226 & 715.43 \\
    
      \hline
    \end{tabular}\caption{Pipeline execution characteristics}\label{table:pipelines}
    \end{table*}
    \subsection{Controls}
    Our objective is to measure the speedup brought by Sea on real neuroimaging pipelines.
    Although Sea is agnostic to the internals of the pipelines and their output results, incorrect parameter
    selection and resulting outputs may result in I/O patterns that would not necessarily be observed had the
    pipeline been executed properly.
    
    Visual QC on the pipeline to ensure it was correctly registered and skull-striped to the template brain (MNI template). 
    
    To validate that the outputs produced were sufficiently correct, we overlayed the output on the input image.
    This allowed up to see if steps such as skull-stripping and registration were correctly performed on the data.
    We repeated this step for all datasets and all pipelines processing a single image. In addition, we compared output
    number of files and output size to ensure that the pipeline was not producing different outputs depending on the file
    system used.
    
    % compare md5sums of uncompressed files.
    
    \subsection{Infrastructure}
    
    Our experiments were exected on two different clusters: 1) a local cluster and 2) Compute Canada's Beluga cluster.
    The local cluster comprised of 8 compute nodes with 256GB memory and 125GB tmpfs, running Centos 8. Storage on the local
    cluster was comprised of 4 Lustre ZFS storage nodes with 44 HDD OSTs and 1 MDS with a single MDT. The
    storage nodes were connected to the compute nodes via a 20Gbps ethernet connection.
    The cluster's hardware is similar to that of Beluga.
    
    The Beluga cluster
    
    %Busy writers
    \section{Results}
    
    
    \section{Discussion}
    \subsection{Significance of speedups with Sea}
    
    \subsection{Other use cases}
    writes a lot but it's actually more complicated. When dataset increases
    \subsection{Predicting speedups}
     difficulties with summarizing speedups into guidelines.
     variability is important and it's coming from the interaction between infrastructure and application.
     Benefits of sea depend on parameters such as load of the filesystem which is multifold and the application compute and
     file system configuration. 
     page cache
     examples:
      - large dataset - > more computation
      - data access patterns and bursty-ness
      - small datasets + metadata accesses
    
    \subsection{Libc interception of neuroimaging pipelines}
    
    As demonstrated by our Sea experiments, we have found that libc interception is effective at capturing all file system
    calls executed by the applications. This means that Sea is compatible with the most prominently used neuroimaging
    preprocessing pipelines. Although we have not extensively against all possible parameters and toolbox applications, we
    assume that use of glibc within the toolboxes themselves are likely to be consistent.
    
    The ability to use libc interception over more common alternative methods such as kernel-based or fuse-based filesystems
    was essential in ensuring ease-of-use and minimal performance overheads. 
    
    opportunities for other optimizations
    
    Sea may be beneficial
    \subsection{Should Sea always be used?}
    
    user
    
    sysadmin
    
    write intensive
    
    
    
    \subsection{Effective use of available cluster resources}
    \subsection{Prefetching, flushing and eviction with Sea}
    \subsection{Comparisons to best case scenario}
    
    talk about testing
    
    
    Flushing - rely on modification time
    \section{Conclusion}